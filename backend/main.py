#!/usr/bin/env python3
"""
Backend FastAPI para aplicaci√≥n de superresoluci√≥n
Versi√≥n modular con arquitectura limpia
"""

from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from typing import Optional, List, Dict, Any
import logging
import asyncio
import uvicorn
from pathlib import Path

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Importar m√≥dulos propios
from models.config import MODEL_CONFIGS, get_available_models, validate_model_structure
from models.loader import model_loader
from processing.image_processor import image_processor
from utils.image_utils import image_utils
from utils.quality_evaluator import quality_evaluator

# Crear aplicaci√≥n FastAPI
app = FastAPI(
    title="Microscopy Super-Resolution API",
    version="1.0.0",
    description="API para superresoluci√≥n de im√°genes de microscop√≠a usando ESRGAN, SwinIR y EDSR"
)

# Configurar CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Modelos de datos Pydantic
class ModelInfo(BaseModel):
    name: str
    type: str
    architecture: str
    input_size: int
    output_size: int
    available: bool
    checkpoint_iter: Optional[int] = None
    discriminator: Optional[str] = None

class ProcessPatchRequest(BaseModel):
    model_name: str
    x: int = 0
    y: int = 0
    width: int = 256
    height: int = 256

class SequentialUpsamplingRequest(BaseModel):
    architecture: str
    start_size: int
    target_scale: int
    x: int = 0
    y: int = 0
    width: int = 256
    height: int = 256

class EvaluateQualityRequest(BaseModel):
    calculate_perceptual: bool = True

# Event handlers
@app.on_event("startup")
async def startup_event():
    """Inicializaci√≥n de la aplicaci√≥n"""
    logger.info("üöÄ Iniciando Microscopy Super-Resolution API...")
    
    # Validar estructura de modelos
    validate_model_structure()
    
    # Cargar modelos disponibles
    logger.info("üì¶ Cargando modelos disponibles...")
    load_results = model_loader.load_all_available_models()
    
    loaded_count = sum(load_results.values())
    total_count = len(load_results)
    
    if loaded_count == 0:
        logger.warning("‚ö†Ô∏è  No se cargaron modelos. Verifica la estructura de archivos.")
    else:
        logger.info(f"‚úÖ API iniciada correctamente - {loaded_count}/{total_count} modelos cargados")

@app.on_event("shutdown")
async def shutdown_event():
    """Limpieza al cerrar la aplicaci√≥n"""
    logger.info("üõë Cerrando Microscopy Super-Resolution API...")

# Rutas principales
@app.get("/")
async def root():
    """Endpoint ra√≠z con informaci√≥n de la API"""
    loaded_models = model_loader.get_loaded_models()
    memory_info = model_loader.get_memory_usage()
    
    return {
        "message": "Microscopy Super-Resolution API",
        "version": "1.0.0",
        "status": "running",
        "models_loaded": len(loaded_models),
        "device": memory_info.get("device", "unknown"),
        "endpoints": {
            "models": "/models",
            "process_patch": "/process_patch",
            "process_sequential": "/process_sequential",
            "health": "/health"
        }
    }

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "timestamp": str(asyncio.get_event_loop().time()),
        "memory": model_loader.get_memory_usage()
    }

@app.get("/models", response_model=List[ModelInfo])
async def get_models():
    """Obtiene lista de todos los modelos configurados"""
    models_info = []
    loaded_models = model_loader.get_loaded_models()
    
    for model_name, config in MODEL_CONFIGS.items():
        models_info.append(ModelInfo(
            name=model_name,
            type=config["type"],
            architecture=config["architecture"],
            input_size=config["input_size"],
            output_size=config["output_size"],
            available=model_name in loaded_models,
            checkpoint_iter=config.get("checkpoint_iter"),
            discriminator=config.get("discriminator")
        ))
    
    return models_info

@app.get("/models/{architecture}")
async def get_models_by_architecture(architecture: str):
    """Obtiene modelos filtrados por arquitectura"""
    from models.config import get_models_by_architecture
    
    arch_models = get_models_by_architecture(architecture)
    if not arch_models:
        raise HTTPException(status_code=404, detail=f"Arquitectura {architecture} no encontrada")
    
    loaded_models = model_loader.get_loaded_models()
    models_info = []
    
    for model_name, config in arch_models.items():
        models_info.append({
            "name": model_name,
            "available": model_name in loaded_models,
            "input_size": config["input_size"],
            "output_size": config["output_size"],
            "scale": config["scale"]
        })
    
    return {
        "architecture": architecture,
        "models": models_info,
        "total_models": len(models_info),
        "available_models": sum(1 for m in models_info if m["available"])
    }

@app.post("/process_patch")
async def process_patch(
    file: UploadFile = File(...),
    model_name: str = "esrgan_256_512",
    x: int = 0,
    y: int = 0,
    width: int = 256,
    height: int = 256
):
    """Procesa un parche espec√≠fico de la imagen"""
    
    # Validar que el modelo est√© cargado
    if not model_loader.is_loaded(model_name):
        raise HTTPException(
            status_code=400, 
            detail=f"Modelo {model_name} no est√° disponible"
        )
    
    try:
        # Leer imagen
        contents = await file.read()
        image = image_utils.bytes_to_image(contents)
        
        if image is None:
            raise HTTPException(status_code=400, detail="No se pudo procesar la imagen")
        
        # Extraer parche
        patch = image_processor.extract_patch(image, x, y, width, height)
        
        # Procesar parche
        enhanced_patch = image_processor.process_single_patch(patch, model_name)
        
        if enhanced_patch is None:
            raise HTTPException(status_code=500, detail="Error procesando el parche")
        
        # Convertir a base64
        original_b64 = image_utils.image_to_base64(patch)
        enhanced_b64 = image_utils.image_to_base64(enhanced_patch)
        
        # Obtener informaci√≥n del modelo
        model_info = model_loader.get_model(model_name)
        
        return JSONResponse({
            "success": True,
            "original_patch": original_b64,
            "enhanced_patch": enhanced_b64,
            "original_size": f"{patch.shape[1]}x{patch.shape[0]}",
            "enhanced_size": f"{enhanced_patch.shape[1]}x{enhanced_patch.shape[0]}",
            "model_used": model_name,
            "architecture": model_info["config"]["architecture"],
            "scale_factor": model_info["config"]["scale"],
            "coordinates": {"x": x, "y": y, "width": width, "height": height}
        })
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error procesando parche: {e}")
        raise HTTPException(status_code=500, detail=f"Error interno: {str(e)}")

@app.post("/process_sequential")
async def process_sequential_upsampling(
    file: UploadFile = File(...),
    architecture: str = "ESRGAN",
    start_size: int = 256,
    target_scale: int = 4,
    x: int = 0,
    y: int = 0,
    width: int = 256,
    height: int = 256,
    evaluate_quality: bool = False
):
    """Procesa upsampling secuencial para alcanzar factores de escala altos"""
    
    try:
        # Leer imagen
        contents = await file.read()
        image = image_utils.bytes_to_image(contents)
        
        if image is None:
            raise HTTPException(status_code=400, detail="No se pudo procesar la imagen")
        
        # Extraer parche
        patch = image_processor.extract_patch(image, x, y, width, height)
        original_patch = patch.copy()  # Guardar original para evaluaci√≥n
        
        # Redimensionar a start_size si es necesario
        if patch.shape[0] != start_size or patch.shape[1] != start_size:
            patch = image_utils.resize_image(patch, (start_size, start_size))
        
        # Procesar upsampling secuencial
        result = image_processor.process_sequential_upsampling(
            patch, start_size, target_scale, architecture
        )
        
        if not result or not result["success"]:
            raise HTTPException(
                status_code=400, 
                detail=f"No se puede alcanzar x{target_scale} con {architecture}"
            )
        
        # Preparar respuesta base
        response_data = {
            "success": True,
            "architecture": architecture,
            "target_scale": target_scale,
            "upsampling_path": result["upsampling_path"],
            "original_size": result["original_size"],
            "final_size": result["final_size"],
            "steps": [],
            "quality_metrics": None
        }
        
        # Convertir im√°genes de cada paso a base64
        for step in result["steps"]:
            step_data = {
                "step": step["step"],
                "model_name": step["model_name"],
                "input_size": step["input_size"],
                "output_size": step["output_size"],
                "enhanced_patch": image_utils.image_to_base64(step["enhanced_patch"])
            }
            response_data["steps"].append(step_data)
        
        # Imagen original y final
        response_data["original_patch"] = image_utils.image_to_base64(result["original_patch"])
        response_data["final_result"] = image_utils.image_to_base64(result["final_result"])
        
        # Evaluaci√≥n de calidad opcional
        if evaluate_quality and target_scale >= 2:
            try:
                # Para evaluaci√≥n, redimensionar original al tama√±o final para comparaci√≥n
                final_result = result["final_result"]
                original_resized = image_utils.resize_image(
                    original_patch, 
                    (final_result.shape[1], final_result.shape[0])
                )
                
                quality_results = quality_evaluator.evaluate_image_quality(
                    original=original_resized,
                    enhanced=final_result,
                    calculate_perceptual=True
                )
                
                if quality_results["evaluation_success"]:
                    interpretation = quality_evaluator.get_quality_interpretation(quality_results)
                    response_data["quality_metrics"] = {
                        "psnr": quality_results["psnr"],
                        "ssim": quality_results["ssim"],
                        "perceptual_index": quality_results["perceptual_index"],
                        "interpretation": interpretation,
                        "kimianet_used": quality_evaluator.is_kimianet_available()
                    }
                    logger.info(f"Evaluaci√≥n de calidad completada - PSNR: {quality_results['psnr']:.4f}")
                
            except Exception as e:
                logger.warning(f"Error en evaluaci√≥n de calidad: {e}")
                response_data["quality_metrics"] = {"error": str(e)}
        
        return JSONResponse(response_data)
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error en procesamiento secuencial: {e}")
        raise HTTPException(status_code=500, detail=f"Error interno: {str(e)}")

@app.get("/upsampling_path/{architecture}/{start_size}/{target_scale}")
async def get_upsampling_path(architecture: str, start_size: int, target_scale: int):
    """Obtiene la ruta de upsampling para par√°metros espec√≠ficos"""
    from models.config import get_upsampling_path
    
    target_size = start_size * target_scale
    path = get_upsampling_path(start_size, target_size, architecture)
    
    if not path:
        raise HTTPException(
            status_code=400,
            detail=f"No se puede alcanzar x{target_scale} desde {start_size} con {architecture}"
        )
    
    # Obtener detalles de cada paso
    path_details = []
    current_size = start_size
    
    for model_name in path:
        model_config = MODEL_CONFIGS[model_name]
        path_details.append({
            "model_name": model_name,
            "input_size": current_size,
            "output_size": current_size * 2,
            "architecture": model_config["architecture"],
            "available": model_loader.is_loaded(model_name)
        })
        current_size *= 2
    
    return {
        "architecture": architecture,
        "start_size": start_size,
        "target_scale": target_scale,
        "target_size": target_size,
        "path": path,
        "path_details": path_details,
        "steps_required": len(path),
        "all_models_available": all(detail["available"] for detail in path_details)
    }

@app.post("/load_model/{model_name}")
async def load_model(model_name: str):
    """Carga un modelo espec√≠fico"""
    if model_name not in MODEL_CONFIGS:
        raise HTTPException(status_code=404, detail=f"Modelo {model_name} no encontrado")
    
    if model_loader.is_loaded(model_name):
        return {"message": f"Modelo {model_name} ya est√° cargado", "success": True}
    
    success = model_loader.load_model(model_name)
    
    if success:
        return {"message": f"Modelo {model_name} cargado correctamente", "success": True}
    else:
        raise HTTPException(status_code=500, detail=f"Error cargando modelo {model_name}")

@app.delete("/unload_model/{model_name}")
async def unload_model(model_name: str):
    """Descarga un modelo de la memoria"""
    if not model_loader.is_loaded(model_name):
        raise HTTPException(status_code=404, detail=f"Modelo {model_name} no est√° cargado")
    
    success = model_loader.unload_model(model_name)
    
    if success:
        return {"message": f"Modelo {model_name} descargado", "success": True}
    else:
        raise HTTPException(status_code=500, detail=f"Error descargando modelo {model_name}")

@app.get("/stats")
async def get_stats():
    """Obtiene estad√≠sticas del sistema"""
    return {
        "processing_stats": image_processor.get_processing_stats(),
        "memory_usage": model_loader.get_memory_usage(),
        "loaded_models": list(model_loader.get_loaded_models().keys()),
        "available_models": len(get_available_models()),
        "total_configured_models": len(MODEL_CONFIGS),
        "kimianet_available": quality_evaluator.is_kimianet_available()
    }

@app.post("/evaluate_quality")
async def evaluate_image_quality(
    original_file: UploadFile = File(...),
    enhanced_file: UploadFile = File(...),
    calculate_perceptual: bool = True
):
    """Eval√∫a la calidad de una imagen procesada vs original usando KimiaNet"""
    
    try:
        # Leer im√°genes
        original_contents = await original_file.read()
        enhanced_contents = await enhanced_file.read()
        
        original_image = image_utils.bytes_to_image(original_contents)
        enhanced_image = image_utils.bytes_to_image(enhanced_contents)
        
        if original_image is None or enhanced_image is None:
            raise HTTPException(status_code=400, detail="No se pudieron procesar las im√°genes")
        
        # Evaluar calidad
        results = quality_evaluator.evaluate_image_quality(
            original=original_image,
            enhanced=enhanced_image,
            calculate_perceptual=calculate_perceptual
        )
        
        if not results["evaluation_success"]:
            raise HTTPException(status_code=500, detail=f"Error en evaluaci√≥n: {results.get('error_message', 'Error desconocido')}")
        
        # Obtener interpretaci√≥n
        interpretation = quality_evaluator.get_quality_interpretation(results)
        
        return JSONResponse({
            "success": True,
            "metrics": {
                "psnr": results["psnr"],
                "ssim": results["ssim"],
                "perceptual_index": results["perceptual_index"]
            },
            "interpretation": interpretation,
            "kimianet_used": quality_evaluator.is_kimianet_available(),
            "original_image_info": image_utils.get_image_info(original_image),
            "enhanced_image_info": image_utils.get_image_info(enhanced_image)
        })
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error en evaluaci√≥n de calidad: {e}")
        raise HTTPException(status_code=500, detail=f"Error interno: {str(e)}")

@app.get("/kimianet_status")
async def get_kimianet_status():
    """Obtiene el estado de KimiaNet"""
    is_available = quality_evaluator.is_kimianet_available()
    
    status_info = {
        "available": is_available,
        "weights_path": quality_evaluator.kimianet_weights_path if quality_evaluator.kimianet_evaluator else None,
        "model_loaded": quality_evaluator.kimianet_evaluator is not None
    }
    
    if is_available:
        status_info["message"] = "KimiaNet est√° disponible para evaluaci√≥n perceptual"
    else:
        status_info["message"] = "KimiaNet no est√° disponible - verificar pesos del modelo"
    
    return status_info

if __name__ == "__main__":
    uvicorn.run(
        "main:app",
        host="0.0.0.0", 
        port=8000,
        reload=True,
        log_level="info"
    )