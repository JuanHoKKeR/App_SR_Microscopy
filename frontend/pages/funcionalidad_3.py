"""
Funcionalidad 3: EvaluaciÃ³n y ComparaciÃ³n de Modelos
Permite comparar diferentes modelos y mÃ©tricas de calidad
"""

import streamlit as st
import numpy as np
import pandas as pd
from components.ui_config import show_info_box, show_metric_card, show_comparison_layout
from components.api_client import APIClient

def funcionalidad_3_page(api_client: APIClient):
    """PÃ¡gina de funcionalidad 3 - EvaluaciÃ³n y comparaciÃ³n"""
    
    st.markdown('<h2 class="sub-header">ğŸ“Š Funcionalidad 3: EvaluaciÃ³n y ComparaciÃ³n</h2>', unsafe_allow_html=True)
    
    show_info_box("""
    **Esta funcionalidad te permite:**
    - ğŸ“Š Comparar diferentes arquitecturas de super-resoluciÃ³n
    - ğŸ” Evaluar mÃ©tricas de calidad especializadas
    - ğŸ“ˆ AnÃ¡lisis visual con mapas de diferencias
    - ğŸ“‹ Generar reportes comparativos detallados
    - ğŸ§  EvaluaciÃ³n perceptual con KimiaNet
    """)
    
    # Obtener modelos disponibles
    available_models = api_client.get_available_models()
    if not available_models:
        st.error("âŒ No se pudieron cargar los modelos disponibles")
        return
    
    # Verificar estado de KimiaNet
    kimianet_status = api_client.get_kimianet_status()
    kimianet_available = kimianet_status and kimianet_status.get("available", False)
    
    # Tabs para diferentes tipos de evaluaciÃ³n
    tab1, tab2, tab3, tab4 = st.tabs([
        "ğŸ” EvaluaciÃ³n Individual", 
        "âš–ï¸ ComparaciÃ³n MÃºltiple", 
        "ğŸ“ˆ AnÃ¡lisis Visual", 
        "ğŸ“‹ Reportes"
    ])
    
    with tab1:
        individual_evaluation_tab(api_client, available_models, kimianet_available)
    
    with tab2:
        multiple_comparison_tab(api_client, available_models)
    
    with tab3:
        visual_analysis_tab(api_client)
    
    with tab4:
        reports_tab()

def individual_evaluation_tab(api_client, available_models, kimianet_available):
    """Tab para evaluaciÃ³n individual"""
    st.markdown("### ğŸ” EvaluaciÃ³n Individual de Modelos")
    
    show_info_box("""
    **EvalÃºa un modelo especÃ­fico** cargando una imagen LR y su correspondiente HR, 
    o permitiendo que el sistema genere la predicciÃ³n y compare con ground truth.
    """, "info")
    
    # Modo de evaluaciÃ³n
    eval_mode = st.radio(
        "ğŸ¯ Modo de EvaluaciÃ³n:",
        [
            "ğŸ“ Cargar LR + HR (tengo ambas imÃ¡genes)",
            "ğŸ¯ Cargar HR + Generar predicciÃ³n (solo tengo imagen HR)"
        ],
        help="Selecciona segÃºn las imÃ¡genes que tengas disponibles"
    )
    
    if eval_mode.startswith("ğŸ“"):
        evaluation_with_both_images(api_client, available_models, kimianet_available)
    else:
        evaluation_with_prediction(api_client, available_models, kimianet_available)

def evaluation_with_both_images(api_client, available_models, kimianet_available):
    """EvaluaciÃ³n cuando se tienen ambas imÃ¡genes"""
    st.markdown("#### ğŸ“ Cargar ImÃ¡genes LR y HR")
    
    col1, col2 = st.columns(2)
    
    with col1:
        lr_file = st.file_uploader(
            "ğŸ”½ Imagen de Baja ResoluciÃ³n (LR):",
            type=['png', 'jpg', 'jpeg', 'tiff', 'bmp'],
            key="lr_upload"
        )
        
        if lr_file:
            st.image(lr_file, caption="Imagen LR", use_column_width=True)
    
    with col2:
        hr_file = st.file_uploader(
            "ğŸ”¼ Imagen de Alta ResoluciÃ³n (HR):",
            type=['png', 'jpg', 'jpeg', 'tiff', 'bmp'],
            key="hr_upload"
        )
        
        if hr_file:
            st.image(hr_file, caption="Imagen HR", use_column_width=True)
    
    if lr_file and hr_file:
        # ConfiguraciÃ³n de evaluaciÃ³n
        st.markdown("#### âš™ï¸ ConfiguraciÃ³n de EvaluaciÃ³n")
        
        col1, col2 = st.columns(2)
        
        with col1:
            # SelecciÃ³n de modelos a evaluar
            architectures = list(set([model["architecture"] for model in available_models if model["available"]]))
            
            selected_architectures = st.multiselect(
                "ğŸ—ï¸ Arquitecturas a Evaluar:",
                architectures,
                default=architectures[:1] if architectures else [],
                help="Puedes seleccionar mÃºltiples arquitecturas para comparar"
            )
        
        with col2:
            # MÃ©tricas a calcular
            st.markdown("**ğŸ“Š MÃ©tricas a Calcular:**")
            
            calculate_psnr = st.checkbox("ğŸ“ˆ PSNR", value=True)
            calculate_ssim = st.checkbox("ğŸ” SSIM", value=True)
            calculate_msssim = st.checkbox("ğŸ¯ MS-SSIM", value=False, help="Solo para imÃ¡genes grandes")
            
            if kimianet_available:
                calculate_perceptual = st.checkbox("ğŸ§  Ãndice Perceptual (KimiaNet)", value=True)
                st.success("âœ… KimiaNet disponible")
            else:
                calculate_perceptual = st.checkbox("ğŸ§  Ãndice Perceptual (KimiaNet)", value=False, disabled=True)
                st.warning("âš ï¸ KimiaNet no disponible")
        
        # BotÃ³n de evaluaciÃ³n
        if st.button("ğŸ” Evaluar Modelos", type="primary") and selected_architectures:
            # Placeholder para evaluaciÃ³n
            st.info("ğŸš§ **EvaluaciÃ³n en desarrollo**")
            
            # Simular resultados
            show_evaluation_results_placeholder(selected_architectures)

def evaluation_with_prediction(api_client, available_models, kimianet_available):
    """EvaluaciÃ³n generando predicciÃ³n desde HR"""
    st.markdown("#### ğŸ¯ Generar PredicciÃ³n y Evaluar")
    
    hr_file = st.file_uploader(
        "ğŸ”¼ Imagen de Alta ResoluciÃ³n (Ground Truth):",
        type=['png', 'jpg', 'jpeg', 'tiff', 'bmp'],
        key="hr_pred_upload"
    )
    
    if hr_file:
        col1, col2 = st.columns(2)
        
        with col1:
            st.image(hr_file, caption="Imagen HR (Ground Truth)", use_column_width=True)
        
        with col2:
            st.markdown("**âš™ï¸ ConfiguraciÃ³n:**")
            
            # ConfiguraciÃ³n de degradaciÃ³n
            degradation_factor = st.selectbox(
                "ğŸ“‰ Factor de DegradaciÃ³n:",
                [2, 4, 8],
                help="Factor para generar imagen LR desde HR"
            )
            
            # SelecciÃ³n de arquitectura
            architectures = list(set([model["architecture"] for model in available_models if model["available"]]))
            architecture = st.selectbox("ğŸ—ï¸ Arquitectura:", architectures)
            
            # BotÃ³n de procesamiento
            if st.button("ğŸ¯ Generar y Evaluar", type="primary"):
                st.info("ğŸš§ **Funcionalidad en desarrollo**")

def multiple_comparison_tab(api_client, available_models):
    """Tab para comparaciÃ³n mÃºltiple"""
    st.markdown("### âš–ï¸ ComparaciÃ³n MÃºltiple de Arquitecturas")
    
    show_info_box("""
    **Compara mÃºltiples arquitecturas** en el mismo conjunto de imÃ¡genes 
    para obtener anÃ¡lisis estadÃ­sticos y rankings de rendimiento.
    """, "info")
    
    # ConfiguraciÃ³n de comparaciÃ³n
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("**ğŸ“ Dataset de EvaluaciÃ³n:**")
        
        dataset_mode = st.radio(
            "Modo de Dataset:",
            ["ğŸ“‚ Cargar Carpeta de ImÃ¡genes", "ğŸ“‹ Cargar Lista de Pares LR/HR"],
            help="Selecciona cÃ³mo proporcionarÃ¡s las imÃ¡genes de prueba"
        )
        
        # Placeholder para carga de dataset
        if dataset_mode.startswith("ğŸ“‚"):
            st.file_uploader(
                "Cargar carpeta comprimida (.zip):",
                type=['zip'],
                help="Zip con imÃ¡genes LR y HR en carpetas separadas"
            )
        else:
            st.file_uploader(
                "Cargar archivo CSV con rutas:",
                type=['csv'],
                help="CSV con columnas: lr_path, hr_path"
            )
    
    with col2:
        st.markdown("**ğŸ—ï¸ Arquitecturas a Comparar:**")
        
        architectures = list(set([model["architecture"] for model in available_models if model["available"]]))
        
        selected_architectures = st.multiselect(
            "Selecciona arquitecturas:",
            architectures,
            default=architectures if len(architectures) <= 3 else architectures[:3],
            help="MÃ¡ximo 5 arquitecturas para comparaciÃ³n efectiva"
        )
        
        if len(selected_architectures) > 5:
            st.warning("âš ï¸ Muchas arquitecturas seleccionadas. Recomendado: mÃ¡ximo 5.")
    
    # ConfiguraciÃ³n de mÃ©tricas
    st.markdown("**ğŸ“Š ConfiguraciÃ³n de MÃ©tricas:**")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown("**MÃ©tricas BÃ¡sicas:**")
        metrics_basic = st.multiselect(
            "Seleccionar:",
            ["PSNR", "SSIM", "MSE"],
            default=["PSNR", "SSIM"]
        )
    
    with col2:
        st.markdown("**MÃ©tricas Avanzadas:**")
        metrics_advanced = st.multiselect(
            "Seleccionar:",
            ["MS-SSIM", "LPIPS", "FID"],
            default=["MS-SSIM"]
        )
    
    with col3:
        st.markdown("**MÃ©tricas Especializadas:**")
        metrics_specialized = st.multiselect(
            "Seleccionar:",
            ["Ãndice Perceptual (KimiaNet)", "NIQE", "BRISQUE"],
            default=["Ãndice Perceptual (KimiaNet)"]
        )
    
    # BotÃ³n de comparaciÃ³n
    if st.button("âš–ï¸ Iniciar ComparaciÃ³n", type="primary") and selected_architectures:
        st.info("ğŸš§ **ComparaciÃ³n mÃºltiple en desarrollo**")
        
        # Placeholder para resultados de comparaciÃ³n
        show_comparison_results_placeholder(selected_architectures)

def visual_analysis_tab(api_client):
    """Tab para anÃ¡lisis visual"""
    st.markdown("### ğŸ“ˆ AnÃ¡lisis Visual de Diferencias")
    
    show_info_box("""
    **AnÃ¡lisis visual especializado** que incluye mapas de diferencias, 
    anÃ¡lisis de gradientes, y visualizaciÃ³n de artefactos.
    """, "info")
    
    # Cargar imÃ¡genes para anÃ¡lisis
    col1, col2 = st.columns(2)
    
    with col1:
        original_file = st.file_uploader(
            "ğŸ”¼ Imagen Original/Ground Truth:",
            type=['png', 'jpg', 'jpeg', 'tiff', 'bmp'],
            key="visual_original"
        )
        
        if original_file:
            st.image(original_file, caption="Original", use_column_width=True)
    
    with col2:
        processed_file = st.file_uploader(
            "ğŸ¯ Imagen Procesada:",
            type=['png', 'jpg', 'jpeg', 'tiff', 'bmp'],
            key="visual_processed"
        )
        
        if processed_file:
            st.image(processed_file, caption="Procesada", use_column_width=True)
    
    if original_file and processed_file:
        st.markdown("### ğŸ” Tipos de AnÃ¡lisis Visual")
        
        analysis_types = st.multiselect(
            "Selecciona anÃ¡lisis a realizar:",
            [
                "ğŸ“Š Mapa de Diferencias Absolutas",
                "ğŸŒˆ Mapa de Diferencias con Colores",
                "ğŸ“ˆ AnÃ¡lisis de Gradientes",
                "ğŸ” DetecciÃ³n de Artefactos",
                "ğŸ“‹ Histograma de Diferencias",
                "ğŸ¯ AnÃ¡lisis de Regiones de InterÃ©s"
            ],
            default=["ğŸ“Š Mapa de Diferencias Absolutas", "ğŸŒˆ Mapa de Diferencias con Colores"]
        )
        
        if st.button("ğŸ“ˆ Generar AnÃ¡lisis Visual", type="primary"):
            st.info("ğŸš§ **AnÃ¡lisis visual en desarrollo**")
            
            # Placeholder para anÃ¡lisis visual
            show_visual_analysis_placeholder()

def reports_tab():
    """Tab para generar reportes"""
    st.markdown("### ğŸ“‹ GeneraciÃ³n de Reportes")
    
    show_info_box("""
    **Genera reportes detallados** de tus evaluaciones con grÃ¡ficos, 
    tablas comparativas y anÃ¡lisis estadÃ­sticos.
    """, "info")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("**ğŸ“Š Tipo de Reporte:**")
        
        report_type = st.selectbox(
            "Selecciona tipo:",
            [
                "ğŸ“‹ Reporte de EvaluaciÃ³n Individual",
                "âš–ï¸ Reporte de ComparaciÃ³n MÃºltiple",
                "ğŸ“ˆ Reporte de AnÃ¡lisis Visual",
                "ğŸ“Š Reporte Completo (Todo incluido)"
            ]
        )
        
        # ConfiguraciÃ³n de reporte
        include_images = st.checkbox("ğŸ–¼ï¸ Incluir imÃ¡genes en reporte", value=True)
        include_charts = st.checkbox("ğŸ“Š Incluir grÃ¡ficos y tablas", value=True)
        include_raw_data = st.checkbox("ğŸ“‹ Incluir datos en bruto", value=False)
    
    with col2:
        st.markdown("**ğŸ“ Formato de Salida:**")
        
        output_format = st.selectbox(
            "Formato:",
            ["PDF", "HTML", "Word (.docx)", "Excel (.xlsx)"],
            help="PDF recomendado para reportes profesionales"
        )
        
        # ConfiguraciÃ³n adicional
        report_name = st.text_input(
            "ğŸ“ Nombre del reporte:",
            value="Evaluacion_SuperResolucion",
            help="Sin espacios ni caracteres especiales"
        )
        
        # Metadatos
        author = st.text_input("ğŸ‘¤ Autor:", value="Usuario")
        institution = st.text_input("ğŸ¢ InstituciÃ³n:", value="")
    
    # BotÃ³n de generaciÃ³n
    if st.button("ğŸ“‹ Generar Reporte", type="primary"):
        st.info("ğŸš§ **GeneraciÃ³n de reportes en desarrollo**")
        
        st.markdown(f"""
        **Reporte configurado:**
        - Tipo: {report_type}
        - Formato: {output_format}
        - Nombre: {report_name}
        - Autor: {author}
        {f"- InstituciÃ³n: {institution}" if institution else ""}
        """)

# Funciones placeholder para mostrar resultados
def show_evaluation_results_placeholder(architectures):
    """Muestra resultados placeholder de evaluaciÃ³n"""
    st.markdown("### ğŸ“Š Resultados de EvaluaciÃ³n")
    
    # Generar datos ficticios
    results_data = []
    for arch in architectures:
        results_data.append({
            "Arquitectura": arch,
            "PSNR (dB)": np.random.uniform(25, 35),
            "SSIM": np.random.uniform(0.8, 0.95),
            "Ãndice Perceptual": np.random.uniform(0.01, 0.1),
            "Tiempo (s)": np.random.uniform(1, 10)
        })
    
    df = pd.DataFrame(results_data)
    st.dataframe(df, use_container_width=True)
    
    # GrÃ¡fico de barras
    st.bar_chart(df.set_index("Arquitectura")[["PSNR (dB)", "SSIM"]])

def show_comparison_results_placeholder(architectures):
    """Muestra resultados placeholder de comparaciÃ³n"""
    st.markdown("### âš–ï¸ Resultados de ComparaciÃ³n")
    
    # Tabla de ranking
    ranking_data = []
    for i, arch in enumerate(architectures):
        ranking_data.append({
            "Ranking": i + 1,
            "Arquitectura": arch,
            "Score Global": np.random.uniform(0.7, 0.95),
            "PSNR Promedio": np.random.uniform(25, 35),
            "SSIM Promedio": np.random.uniform(0.8, 0.95)
        })
    
    df_ranking = pd.DataFrame(ranking_data)
    st.dataframe(df_ranking, use_container_width=True)

def show_visual_analysis_placeholder():
    """Muestra anÃ¡lisis visual placeholder"""
    st.markdown("### ğŸ” AnÃ¡lisis Visual Generado")
    
    # Placeholder para diferentes tipos de anÃ¡lisis
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("**ğŸ“Š Mapa de Diferencias:**")
        st.info("AquÃ­ se mostrarÃ­a el mapa de diferencias absolutas")
    
    with col2:
        st.markdown("**ğŸ“ˆ AnÃ¡lisis de Gradientes:**")
        st.info("AquÃ­ se mostrarÃ­a el anÃ¡lisis de gradientes")
    
    # MÃ©tricas del anÃ¡lisis
    col1, col2, col3 = st.columns(3)
    
    with col1:
        show_metric_card("15.2%", "PÃ­xeles con Diferencia > 10")
    
    with col2:
        show_metric_card("92.3", "Score de Similaridad")
    
    with col3:
        show_metric_card("3", "Artefactos Detectados")